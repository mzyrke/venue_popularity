{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>\"\n",
    "     #Code\n",
    "     +\"div.code_cell{float: left;flex-direction: col;width: 100%;background-color: #fff2e6}\"\n",
    "     +\"div.code_cell.input {width: 100%;background-color: #fff2e6}\"\n",
    "     +\"div.output_wrapper {width: 100%;float: left;background-color: #e6e6ff}\"\n",
    "     +\"div.code_cell> div.input > div.inner_cell > div.input_area {background-color: GhostWhite}\"\n",
    "     #+\"div.code_cell .cm-comment{background-color: #e6fffa}\"\n",
    "     #Text\n",
    "     +\"div.text_cell{width: 33.3%;float: left;background-color: #EEE}\"\n",
    "     +\"div.text_cell_render code{background-color: #66ccff70}\"\n",
    "     +\"div.text_cell_render cm-comment{background-color: #e6e6ff}\"\n",
    "     +\"div.text_cell_render > pre {background-color: Azure}\"\n",
    "     +\"div.text_cell_render > pre>code {background-color: #ffffff00}\"\n",
    "     +\"div.text_cell div.prompt {display: none;}</style>\")\n",
    "''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# ML: Predicting Star Ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Our objective is to predict a new venue's popularity from information available when the venue opens.  We will do this by machine learning from a data set of venue popularities provided by Yelp.  The data set contains meta data about the venue (where it is located, the type of food served, etc.).  It also contains a star rating. Note that the venues are not limited to restaurants. This tutorial will walk you through one way to build a machine learning algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Metrics and scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "For most questions, you are asked to submit your models `predict` method to the grader. The grader uses a test set to evaluate your model's performance against our reference solution, using the $R^2$ score. It **is** possible to receive a score greater than one, indicating that you've beaten our reference model. We compare our model's score on a test set to your score on the same test set. See how high you can go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Download and parse the incoming data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import gzip\n",
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "#from joblib import dump, load\n",
    "from collections import defaultdict\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from collections import defaultdict\n",
    "from  statistics import mean\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector as csel\n",
    "from pandas import Series\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV as gscv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The training data are a series of JSON objects, in a Gzipped file. Python supports Gzipped files natively: [`gzip.open`](https://docs.python.org/3/library/gzip.html) has the same interface as `open`, but handles `.gz` files automatically.\n",
    "\n",
    "The built-in `json` package has a `loads` function that converts a JSON string into a Python dictionary. We could call that once for each row of the file. [`ujson`](http://docs.micropython.org/en/latest/library/ujson.html) has the same interface as the built-in `json` package, but is *substantially* faster (at the cost of non-robust handling of malformed JSON). We will use that inside a list comprehension to get a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import gzip\n",
    "\n",
    "with gzip.open('./data/yelp_dataset.json.gz') as f:\n",
    "    data = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "<mark> In scikit-learn, the labels to be predicted, in this case, the stars, are always kept in a separate data structure than the features. Let's get in this habit now, by creating a separate list of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "\n",
    "star_ratings = [row['stars'] for row in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "A few things to consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "1. The test set used by the grader will be in the same form as `data`. For this miniproject, it will be a list of dictionaries. The models you will build will need to handle data of this type; we'll discuss this more further in the questions.\n",
    "1. <mark> You may find it useful to serialize your trained model using either [`dill`](https://pypi.python.org/pypi/dill) or [`joblib`](http://scikit-learn.org/stable/modules/model_persistence.html). That way, you can reload your model after restarting the Jupyter notebook without needing to retrain it.\n",
    "1. There are obvious mistakes in the data; there is no need to try to correct them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**MY NOTES: PICKLE VS JOBLIB**\n",
    "\n",
    "[stackoverflow](https://stackoverflow.com/questions/12615525/what-are-the-different-use-cases-of-joblib-versus-pickle#:~:text=joblib%20is%20usually%20significantly%20faster,pickling%20using%20zlib%20or%20lz4.)\n",
    "\n",
    "- joblib is usually significantly faster on large numpy arrays because it has a special handling for the array buffers of the numpy datastructure. To find about the implementation details you can have a look at the source code. It can also compress that data on the fly while pickling using zlib or lz4.\n",
    "\n",
    "- joblib also makes it possible to memory map the data buffer of an uncompressed joblib-pickled numpy array when loading it which makes it possible to share memory between processes.\n",
    "\n",
    "- if you don't pickle large numpy arrays, then regular pickle can be significantly faster, especially on large collections of small python objects (e.g. a large dict of str objects) because the pickle module of the standard library is implemented in C while joblib is pure python.\n",
    "\n",
    "- since PEP 574 (Pickle protocol 5) has been merged in Python 3.8, it is now much more efficient (memory-wise and cpu-wise) to pickle large numpy arrays using the standard library. Large arrays in this context means 4GB or more.\n",
    "\n",
    "- But joblib can still be useful with Python 3.8 to load objects that have nested numpy arrays in memory mapped mode with mmap_mode=\"r\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "# Questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 1: city_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "The venues belong to different cities.  You can imagine that the ratings in some cities are probably higher than others.  We wish to build an estimator to make a prediction based on this, but first we need to work out the average rating for each city.  For this problem, create a list of tuples (city name, star rating), one for each city in the data set. There are many ways to do this; please feel free to experiment on your own.  If you get stuck, the steps below attempt to guide you through the process.\n",
    "\n",
    "A simple approach is to go through all of the dictionaries in our array, calculating the sum of the star ratings and the number of venues for each city. At the end, we can just divide the stars by the count to get the average. We could create a separate sum and count variable for each city, but that will get tedious quickly. A better approach is to create a dictionary for each. The key will be the city name, and the value the running sum or running count.\n",
    "\n",
    "One slight annoyance of this approach is that we will have to test whether a key exists in the dictionary before adding to the running tally.  The `collections` module's `defaultdict` class works around this by providing default values for keys that haven't been used. Thus, if we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "star_sum = defaultdict(int)\n",
    "count = defaultdict(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "we can increment any key of `star_sum` or `count` without first worrying whether the key exists. We need to go through the `data` and `star_ratings` list together, which we can do with the `zip` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "for row, stars in zip(data, star_ratings):\n",
    "    # increment the running sum in star_sum\n",
    "    # increment the running count in count\n",
    "    count[row['city']]+=1\n",
    "    star_sum[row['city']]+=stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now we can calculate the average ratings.  Again, a dictionary makes a good container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "avg_stars = dict()\n",
    "for city in star_sum:\n",
    "    # calculate average star rating and store in avg_stars\n",
    "    avg_stars[city]=star_sum[city]/count[city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'picklejar' created successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "#from joblib import dump, load\n",
    "#=============\n",
    "\n",
    "#=============\n",
    "def make_dir(dir_new,dir_parent='./'):\n",
    "    '''\n",
    "    os.path.dirname(os.path.realpath(__file__))\n",
    "    '''\n",
    "    try:\n",
    "        path = os.path.join(dir_parent, dir_new)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        print(\"Directory '%s' created successfully\" %dir_new)\n",
    "        return path\n",
    "    except OSError as error:\n",
    "        print(\"Directory '%s' can not be created\")\n",
    "\n",
    "def pickle_save(data,filename):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(filename, 'wb') as fo:\n",
    "        joblib.dump(data, fo)\n",
    "def pickle_load(filename):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as fo:  \n",
    "        return joblib.load(fo)\n",
    "#=============\n",
    "\n",
    "#============= set params\n",
    "dir_pickles = \"picklejar\"\n",
    "path_pickles=make_dir(dir_pickles)\n",
    "\n",
    "filename_df_ratings=f'{dir_pickles}/df_ratings.joblib'\n",
    "filename_df_ratings_gby_city=f'{dir_pickles}/df_ratings_gby_city.joblib'\n",
    "\n",
    "filename_model_city=f'{dir_pickles}/model_city.joblib'\n",
    "filename_model_latlong=f'{dir_pickles}/model_latlong.joblib'\n",
    "filename_model_cat=f'{dir_pickles}/model_cat.joblib'\n",
    "filename_model_attr=f'{dir_pickles}/model_attr.joblib'\n",
    "\n",
    "filename_data_Xtrain=f'{dir_pickles}/X_train.joblib'\n",
    "filename_data_ytrain=f'{dir_pickles}/y_train.joblib'\n",
    "filename_data_Xtest=f'{dir_pickles}/X_test.joblib'\n",
    "filename_data_ytest=f'{dir_pickles}/y_test.joblib'\n",
    "filename_avg_stars=f'{dir_pickles}/avg_stars.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=============Save the data\n",
    "df_ratings=pd.json_normalize(data)\n",
    "df_ratings_gby_city=df_ratings.groupby('city')\n",
    "\n",
    "avg_stars=df_ratings_gby_city['stars'].mean().to_dict()\n",
    "\n",
    "pickle_save(df_ratings,filename_df_ratings)\n",
    "pickle_save(df_ratings_gby_city,filename_df_ratings_gby_city)\n",
    "pickle_save(avg_stars,filename_avg_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========Load saved data\n",
    "df_ratings = pickle_load(filename_df_ratings)\n",
    "df_ratings_gby_city = pickle_load(filename_df_ratings_gby_city)\n",
    "avg_stars = pickle_load(filename_avg_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There should be 167 different cities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see that we have 167 entries in the dictionary.\n",
    "grader.check(len(avg_stars) == 167)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We can get that list of tuples by converting the returned view object from the `items` method into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('ml__city_avg', list(avg_stars.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 2: city_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, let's build a custom estimator that will make a prediction based solely on the city of a venue.  It is tempting to hard-code the answers from the previous section into this model, but we're going to resist and do things properly.\n",
    "\n",
    "This custom estimator will have a `fit` method.  It will receive `data` as its argument `X` and `star_ratings` as `y`, and should repeat the calculation of the previous problem there.  Then the `predict` method can look up the average rating for the city of each record it receives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from collections import defaultdict\n",
    "from  statistics import mean\n",
    "class CityRegressor(BaseEstimator, RegressorMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.avg_stars = defaultdict(int)\n",
    "        self.ave=0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Store the average rating per city in self.avg_stars\n",
    "        myDict=defaultdict(lambda: [])\n",
    "        for row, stars in zip(X, y):\n",
    "            myDict[row['city']].append(stars)\n",
    "            \n",
    "        self.avg_stars={k: mean(myDict[k]) for k,v in myDict.items()}\n",
    "        self.ave=mean(list(self.avg_stars.values()))\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \n",
    "        return [self.avg_stars[row['city']] if row['city'] in self.avg_stars.keys() else self.ave for row in X ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now we can create an instance of our regressor and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========Load saved model\n",
    "model_city=pickle_load(filename_model_city)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "city_model = CityRegressor()\n",
    "city_model.fit(data, star_ratings)\n",
    "pickle_save(city_model,filename_model_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "And let's see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.6702903946388683, 3.75, 3.75, 3.75, 3.75]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_model.predict(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There is a problem, however.  What happens if we're asked to estimate the rating of a venue in a city that's not in our training set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.6702903946388683, 3.7127593360958087, 3.6457337883959045]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_model.predict([{'city': 'Phoenix'}, {'city': 'Timbuktu'}, {'city': 'Madison'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Your model should always return a number, even if the city was not in the training data. Make sure it does before submitting your model's predict method to the grader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0000\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('ml__city_model', city_model.predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 3: lat_long_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You can imagine that a city-based model might not be sufficiently fine-grained. For example, we know that some neighborhoods are trendier than others.  Use the latitude and longitude of a venue as features that help you understand neighborhood dynamics.\n",
    "\n",
    "Since we need to select the appropriate columns from our dictionaries to build our latitude-longitude model, we will have to use scikit-learn's [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html). However, the `ColumnTransformer` works with either NumPy arrays or pandas data frames. While we can convert our training data into a data frame easily, the test set the grader uses is a list of dictionaries. Thus, our first estimator in our workflow should be a transformer that converts a list of dictionaries into a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "class ToDataFrame(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.X=None\n",
    "        self.y=None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None,**fit_params):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        \n",
    "        print(f'{len(y)} | {y[:10]}' if y is not None else 'y is None!')\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X,y=None):\n",
    "        # Return a pandas data frame from X\n",
    "        #return pd.json_normalize(X)\n",
    "        # compare the above & below: lower one keeps structure (check attribute col)\n",
    "        if isinstance(X,pd.core.frame.DataFrame):\n",
    "            return X\n",
    "        X_new=pd.DataFrame(X)\n",
    "        #print(df.shape)\n",
    "        if y is None:\n",
    "            return X_new  \n",
    "        return X_new\n",
    "    \n",
    "    def fit_transform(self,X,y=None,**fit_params):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self.fit(X,y,**fit_params).transform(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's test out the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is None!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_data_frame = ToDataFrame()\n",
    "X_t = to_data_frame.fit_transform(data[:5])\n",
    "\n",
    "# Check that our transformer properly transform the input data into a data frame\n",
    "grader.check((X_t == pd.DataFrame(data[:5])).all(axis=None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now we are ready to use `ColumnTransformer` and test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "location_columns=['latitude','longitude']\n",
    "    \n",
    "selector = ColumnTransformer([('numeric', 'passthrough', location_columns)])\n",
    "expected = np.array([data[0]['latitude'], data[0]['longitude']])\n",
    "\n",
    "# Check that our selector returns just two columns, the latitude and longitude\n",
    "grader.check((selector.fit_transform(X_t)[0] == expected).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, let's feed the output of the transformer in to a `KNeighborsRegressor`. As a sanity check, we'll test it with the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4. , 4.2, 4. , 3.8, 4.2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor as KNN\n",
    "\n",
    "# Training the model\n",
    "data_transform = to_data_frame.transform(data)\n",
    "#print(len(data_transform))\n",
    "#print(data_transform.head(1))\n",
    "data_transform = selector.fit_transform(data_transform)\n",
    "knn = KNN(n_neighbors=5)\n",
    "knn.fit(data_transform, star_ratings)\n",
    "\n",
    "# Making predictions\n",
    "test_data = data[:5]\n",
    "test_data_transform = to_data_frame.transform(test_data)\n",
    "test_data_transform = selector.transform(test_data_transform)\n",
    "knn.predict(test_data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "We are not ready to submit to the grader; there are two things we still need to do:\n",
    "1. Wrap all the steps necessary to go from our data (list of dictionaries) to predicted ratings\n",
    "1. Determine the optimal value for our predictor's hyperparameter\n",
    "\n",
    "For the first point, we will use a pipeline, ensuring that our model applies all the required transformations given the form of the input data. Remember that a pipeline is made with a list of `(step_name, estimator)` tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector as csel\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('toDF', to_data_frame),\n",
    "    ('keep_pos',selector),\n",
    "    ('knn',knn)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now let's fit and predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37938 | [3.5, 4.0, 4.0, 4.5, 4.0, 1.5, 2.0, 2.5, 4.5, 3.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4. , 4.2, 4. , 3.8, 4.2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(data, star_ratings)\n",
    "pipe.predict(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's now focus on the second point. The `KNeighborsRegressor` takes the `n_neighbors` hyperparameter, which tells it how many nearest neighbors to average together when making a prediction. There is no reason to believe that 5 is the optimum value. We will need to determine a better value for this hyperparameter. A common approach is to use a hyperparameter searching tool such as [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). You may need to refer back to the notebooks about the ways to interface searching tools and pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "You should consider whether the data needs to be shuffled as it might not have been randomized. For example, the data could be ordered by a certain feature or by the labels. If you perform a train/test split with [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split), the data is shuffled by default. However, when using `GridSearchCV`, the folds are not shuffled when you use the default K-folds cross-validation.\n",
    "\n",
    "The code below will plot a rolling mean of the star ratings. Do you need to shuffle the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas import Series\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(Series.rolling(Series(star_ratings), window=1000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Once you've found a good value of `n_neighbors`, submit the model to the grader. Note, \"good\" is a relative measure here. The reference solution has an $R^2$ score of only 0.02. There is just rather little signal available for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import GridSearchCV as gscv\n",
    "from sklearn.model_selection import train_test_split\n",
    "#==============\n",
    "class DataSplitShuffler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,test_size=0.2,random_state=0,shuffle=True):\n",
    "        self.X = None #train\n",
    "        self.y = None #train\n",
    "        self.X_train=None\n",
    "        self.y_train=None\n",
    "        self.X_test=None\n",
    "        self.y_test=None\n",
    "        self.test_size=test_size\n",
    "        self.random_state=random_state\n",
    "        self.shuffle=shuffle    \n",
    "        \n",
    "        \n",
    "    def fit(self, X, y=None,**fit_params):\n",
    "\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            test_size=self.test_size, \n",
    "                                                            random_state=self.random_state,\n",
    "                                                            shuffle=self.shuffle)\n",
    "        #self.y=y_train\n",
    "        self.X_train=X_train\n",
    "        self.y_train=y_train\n",
    "        self.X_test=X_test\n",
    "        self.y_test=y_test\n",
    " \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X,y):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #print(type(self.X_train))\n",
    "        #print(f'{len(self.y_train)} | {self.y_train[:10]}') \n",
    "        #print(type(self))\n",
    "        df1 = pd.DataFrame({\"X_train\": [self.X_train],\n",
    "                            \"y_train\": [self.y_train],\n",
    "                            \"X_test\": [self.X_test],\n",
    "                            \"y_test\": [self.y_test]},\n",
    "            index=[0, 1, 2, 3])\n",
    "        print(f'{type(self.X_train[location_columns])}  | {self.X_train[location_columns].shape}')\n",
    "        print(f'{type(self.y_train)}  | {len(self.y_train)}')\n",
    "        return df1\n",
    "    \n",
    "    def fit_transform(self, X, y, **fit_params):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        return self.fit(X,y,**fit_params).transform(X,y)\n",
    "    \n",
    "    def getData_All(self):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        return self.X_train,self.y_train,self.X_test,self.y_test\n",
    "\n",
    "\n",
    "#==============\n",
    "def param_factory(params, prefix=[], result={}):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    for k,v in params.items():\n",
    "        if isinstance(k,str):\n",
    "            if isinstance(v,dict):\n",
    "                param_factory(v, prefix+[k],result)\n",
    "            else:\n",
    "                prefix_str = '__'.join(prefix+[k])\n",
    "                if not prefix_str in result:\n",
    "                    result[prefix_str] = {}\n",
    "                result[prefix_str] = v\n",
    "        else:\n",
    "            raise TypeError(f'{type(v)} is not permissible in data structure')\n",
    "    return result\n",
    "#=============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "#==============\n",
    "data_prep = Pipeline([\n",
    "    ('toDF', ToDataFrame()),\n",
    "    ('data_split_shuffle',DataSplitShuffler())\n",
    "])\n",
    "\n",
    "params_dss={'test_size':0.2,'random_state':1,'shuffle':True}\n",
    "params_data_prep={'data_split_shuffle':params_dss}\n",
    "#========\n",
    "features_loc=ColumnTransformer(\n",
    "    [('loc', 'passthrough', location_columns)],\n",
    "    remainder='drop')\n",
    "\n",
    "#========\n",
    "pipe_est=Pipeline([('knn', KNN())])\n",
    "\n",
    "params_knn={'n_neighbors': range(1,60), \n",
    "            'weights':['uniform', 'distance'], \n",
    "            'p':range(1,3)}#, 'leaf_size':range(1,15) , 'algorithm': ['ball_tree', 'kd_tree', 'brute']\n",
    "params_pipe_est={'knn':params_knn}\n",
    "\n",
    "\n",
    "param_grid = param_factory(params_pipe_est,[],{})\n",
    "params_gscv={'estimator':pipe_est,\n",
    "             'param_grid':param_grid,\n",
    "             'cv':50,\n",
    "             'n_jobs':50,\n",
    "             'verbose':1}\n",
    "#print(params_gscv.items())\n",
    "#params_gscv=\n",
    "#params_est_gscv={'gs_est':params_gscv}\n",
    "\n",
    "#========\n",
    "params={'data_prep':params_data_prep}#,'est_gscv':params_est_gscv\n",
    "params_master=param_factory(params,[],{})\n",
    "\n",
    "data_prep.set_params(**param_factory(params_data_prep,[],{}))\n",
    "df_train_test=data_prep.fit_transform(data,star_ratings)\n",
    "#('data_prep',data_prep)\n",
    "X_train=df_train_test.loc[1,'X_train']\n",
    "y_train=df_train_test.loc[1,'y_train']\n",
    "\n",
    "X_test=df_train_test.loc[1,'X_test']\n",
    "y_test=df_train_test.loc[1,'y_test']\n",
    "\n",
    "pickle_save(X_train,filename_data_Xtrain)\n",
    "pickle_save(y_train,filename_data_ytrain)\n",
    "pickle_save(X_test,filename_data_Xtest)\n",
    "pickle_save(y_test,filename_data_ytest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_latlong = Pipeline([('toDF', ToDataFrame()), ('f_loc', features_loc), ('est_gscv',gscv(**params_gscv))])#\n",
    "model_latlong.fit(X_train,y_train)\n",
    "\n",
    "pickle_save(model_latlong, filename_model_latlong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========Load saved model\n",
    "#model_latlong=pickle_load(filename_model_latlong)\n",
    "\n",
    "X_train = pickle_load(filename_data_Xtrain)\n",
    "y_train = pickle_load(filename_data_ytrain)\n",
    "X_test = pickle_load(filename_data_Xtest)\n",
    "y_test = pickle_load(filename_data_ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 1.0824\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('ml__lat_long_model', model_latlong.predict)  # Edit to appropriate name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 4: category_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "While location is important, we could also try seeing how predictive the\n",
    "venue's category is. Build an estimator that considers only the `'categories'` field of the data.\n",
    "\n",
    "The categories come as a list of strings, but the scikit-learn's predictors all need numeric input. We ultimately want to create a column in our feature matrix to represent every category. For a given row, only the columns that represent the categories it contains will be filled with a one, otherwise, it will be filled with a zero. The described method is similar to **one-hot encoding**, however, an observation/row can contain more than one \"hot\", non-zero, column.\n",
    "\n",
    "To achieve our encoding plan, we need to use scikit-learn's provides [`DictVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer). This transformer takes a 1-D array of dictionaries and creates a column in the output matrix for each key in the dictionary and fills it with the value associated with it. Missing keys are filled with zeros. However, we need to build a transformer that takes an array of strings and returns an array of dictionaries with keys given by those strings and values of one. For example, it should transform `X_in` into `X_out`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [a]\n",
      "1    [b, c]\n",
      "dtype: object\n",
      "0            {'a': 1}\n",
      "1    {'b': 1, 'c': 1}\n",
      "dtype: object\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "X_in = pd.Series([['a'], ['b', 'c']])\n",
    "X_out = pd.Series([{'a': 1}, {'b': 1, 'c': 1}])\n",
    "\n",
    "\n",
    "print(X_in)\n",
    "print(X_out)\n",
    "print(type(X_in.apply(lambda x: dict(zip(x, [1]*len(x)))).squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "\n",
    "class DictEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X will be a pandas series. Return a pandas series of dictionaries\n",
    "        #print(X.head(2))\n",
    "        return X.squeeze().apply(lambda x: dict(zip(x, [1]*len(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now let's test out that our `DictEncoder` works out as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that DictEncoder transforms a series of list of strings into the expected series of dictionaries\n",
    "grader.check((DictEncoder().fit_transform(X_in) == X_out).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Now, create a pipeline object of the two step transformation for the categories data. Afterwards, create a `ColumnTransformer` object that will use the aforementioned pipeline object to transform the `'categories'` field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "cols_cat=['categories']\n",
    "\n",
    "pipe_cat_encoder = Pipeline([('dictEnc', DictEncoder()),('dv', DictVectorizer())])\n",
    "features_cat=ColumnTransformer([('numeric', pipe_cat_encoder, cols_cat)],remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Finally, create a pipeline object that will\n",
    "1. Convert our list of dictionaries into a data frame\n",
    "1. Select the `'categories'` column and encode the data\n",
    "1. Train a regularized linear model such as `Ridge`\n",
    "\n",
    "There will be a large number of features, one for each category, so there is a significant danger of overfitting. Use cross validation to choose the best regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30350 | [2.5, 4.5, 4.5, 3.5, 3.0, 4.0, 3.0, 4.0, 2.5, 4.0]\n",
      "Fitting 20 folds for each of 22 candidates, totalling 440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend LokyBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:   29.9s\n",
      "[Parallel(n_jobs=50)]: Done 440 out of 440 | elapsed:   44.9s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "\n",
    "\n",
    "\n",
    "pipe_cat_fit_basic = Pipeline([\n",
    "    ('toDF', ToDataFrame()),\n",
    "    ('f_cat',features_cat),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "#pipe_cat_fit_basic.fit(X_train,y_train)\n",
    "\n",
    "#========\n",
    "pipe_cat_est=Pipeline([('ridge', Ridge())])\n",
    "\n",
    "params_ridge={'alpha': np.linspace(0,10,11), 'normalize':[True, False]}\n",
    "params_cat_pipe_est={'ridge':params_ridge}\n",
    "\n",
    "\n",
    "param_cat_grid = param_factory(params_cat_pipe_est,[],{})\n",
    "params_cat_gscv={'estimator':pipe_cat_est,\n",
    "             'param_grid':param_cat_grid,\n",
    "             'cv':20,\n",
    "             'n_jobs':50,\n",
    "             'verbose':1}\n",
    "\n",
    "\n",
    "#======== Fit the Model\n",
    "model_cat = Pipeline([('toDF', ToDataFrame()),('f_cat',features_cat), ('est_cat_gscv',gscv(**params_cat_gscv))])#\n",
    "model_cat.fit(X_train,y_train)\n",
    "pickle_save(model_cat, filename_model_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#========Load saved model\n",
    "model_cat=pickle_load(filename_model_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           categories\n",
      "0        [Hair Salons, Beauty & Spas]\n",
      "1  [Arts & Crafts, Shopping, Framing]\n",
      "==================\n",
      "Your score: 0.9969\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('ml__category_model', model_cat.predict)  # Edit to appropriate name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**Extension:** Some categories (e.g., Restaurants) are not very specific. Others (Japanese sushi) are much more so.  One way to deal with this is with an measure call term frequency-inverse document frequency (tf-idf). Add in a [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) between the `DictVectorizer` and the linear model, and see if that improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 5: attribute_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "There is even more information in the attributes for each venue.  Let's build an estimator based on these.\n",
    "\n",
    "Venues attributes may be nested:\n",
    "```python\n",
    "{\n",
    "  'Attire': 'casual',\n",
    "  'Accepts Credit Cards': True,\n",
    "  'Ambiance': {'casual': False, 'classy': False}\n",
    "}\n",
    "```\n",
    "We wish to encode them in the same manner as our categories data using the `DictVectorizer`. Before we do so, we need to flatten the dictionary to a single level:\n",
    "```python\n",
    "{\n",
    "  'Attire_casual' : 1,\n",
    "  'Accepts Credit Cards': 1,\n",
    "  'Ambiance_casual': 0,\n",
    "  'Ambiance_classy': 0\n",
    "}\n",
    "```\n",
    "Build a custom transformer that flattens the dictionary for the `'attributes'` field. Similar to what was done before, create a model that properly encodes the attribute data and learns to predict the ratings.\n",
    "\n",
    "You may find it difficult to find a single regressor that does well enough. A common solution is to use a linear model to fit the linear part of some data, and use a non-linear model to fit the residual that the linear model can't fit. Build a custom predictor that takes as an argument two other predictors. It should use the first to fit the raw data and the second to fit the residuals of the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV as rscv\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "\n",
    "# Create the transformer to handle the attributes data\n",
    "class DictFlattener(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.X = None \n",
    "        self.y = None \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        rv=[]\n",
    "        #print('BingoDingo')\n",
    "        #print(X.head(1))\n",
    "        for row in X.iterrows():\n",
    "            dict_flat={}\n",
    "            #print(row)\n",
    "            #print(type(row))\n",
    "            #print(len(row))\n",
    "            #row_d=dict(row)\n",
    "            for attr,value in row[1].items():\n",
    "                self.kv_updater(str(attr), value, dict_flat)\n",
    "            rv.append(dict_flat)\n",
    "        return pd.Series(rv)\n",
    "        \n",
    "    \n",
    "    def fit_transform(self, X,y=None):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "    \n",
    "    \n",
    "    def kv_updater(self,key,value,dict_flat):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        if isinstance(value, (bool, int, float)):\n",
    "            dict_flat[key] = int(value)\n",
    "        elif isinstance(value,str):\n",
    "            dict_flat[key+'_'+value]=1\n",
    "        elif isinstance(value, dict):\n",
    "            for key_2,value_2 in value.items():\n",
    "                key_new=key+'_'+str(key_2)\n",
    "                self.kv_updater(key_new,value_2, dict_flat)\n",
    "\n",
    "# ==================Create the linear + non-linear ensemble predictor\n",
    "class EnssembleReg(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, lreg, nreg, params={}):\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        #print(f'regressors are: {lreg} & {nreg}')\n",
    "        self.params=params\n",
    "        self.lreg = lreg\n",
    "        self.nreg = nreg\n",
    "\n",
    "    def fit(self, X,y):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print('EnsReg: Start')\n",
    "        #self.params_2_file(self.params)\n",
    "        self.lreg.fit(X,y) \n",
    "        #print('EnsReg: lr finished')\n",
    "        self.nreg.fit(X, y-self.lreg.predict(X))\n",
    "        print('EnsReg: End')\n",
    "        return self\n",
    "    \n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        #print('EnsReg: Done')\n",
    "        return self.nreg.predict(X)+self.lreg.predict(X)\n",
    "    \n",
    "    '''def get_params(self, deep=True):\n",
    "    \n",
    "    out = dict()\n",
    "    for key in self._get_param_names():\n",
    "        value = getattr(self, key)\n",
    "        if deep and hasattr(value, \"get_params\"):\n",
    "            deep_items = value.get_params().items()\n",
    "            out.update((key + \"__\" + k, val) for k, val in deep_items)\n",
    "        out[key] = value\n",
    "    out\n",
    "    {\"params\": self.params, \n",
    "            'lreg':self.lreg.get_params(), \n",
    "            'nreg': self.nreg.get_params()}\n",
    "    return '''\n",
    "                \n",
    "\n",
    "    def set_params(self,**params):\n",
    "        \n",
    "        \n",
    "        lr_param='lreg__'\n",
    "        nr_param='nreg__'\n",
    "        lr_reg='lregg'\n",
    "        nr_reg='nregg'\n",
    "        \n",
    "        param_lr={}\n",
    "        param_nr={}\n",
    "        \n",
    "        for key, value in params.items():\n",
    "            if lr_param in key:\n",
    "                param_lr[key.split(lr_param)[1]]=value\n",
    "            elif nr_param in key:\n",
    "                param_nr[key.split(nr_param)[1]]=value\n",
    "            elif lr_reg in key:\n",
    "                #print(value)\n",
    "                self.lr=value()\n",
    "            elif nr_reg in key:\n",
    "                #print(value)\n",
    "                self.nr=value()\n",
    "        \n",
    "        print(f'Params -> \\n reg_nr: \\n{param_nr} & \\nreg_lr: \\n{param_lr}')\n",
    "        self.lreg.set_params(**param_lr)\n",
    "        self.nreg.set_params(**param_nr)\n",
    "        \n",
    "        self.params = params\n",
    "        #print(f'at setparams: {self.params}')\n",
    "        return self\n",
    "    \n",
    "    def params_2_file(self, params):\n",
    "        \n",
    "        with open(f'{dir_pickles}/model_attr_param.txt', 'a') as file1:\n",
    "            file1.write(json.dumps(params))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30350 | [2.5, 4.5, 4.5, 3.5, 3.0, 4.0, 3.0, 4.0, 2.5, 4.0]\n",
      "Fitting 20 folds for each of 1 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done   4 out of  20 | elapsed:  2.9min remaining: 11.5min\n",
      "[Parallel(n_jobs=40)]: Done  20 out of  20 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params -> \n",
      " reg_nr: \n",
      "{'max_depth': 20, 'max_features': 60, 'n_estimators': 100} & \n",
      "reg_lr: \n",
      "{}\n",
      "EnsReg: Start\n",
      "EnsReg: End\n"
     ]
    }
   ],
   "source": [
    "#================================ Create the attribute model\n",
    "#========\n",
    "cols_attr=['attributes']\n",
    "\n",
    "pipe_attr_encoder = Pipeline([('dictF',DictFlattener()),('dv', DictVectorizer())]) #, ('dictEnc', DictEncoder())\n",
    "features_attr=ColumnTransformer([('attrs', pipe_attr_encoder, cols_attr)],remainder='drop')\n",
    "\n",
    "#======== Estimator pipe\n",
    "lr=LinearRegression()\n",
    "params_attr_lr={}\n",
    "\n",
    "#lr=Ridge()\n",
    "#params_attr_lr={'alpha': np.linspace(1, 20, 20), 'normalize':[False]} #\n",
    "\n",
    "\n",
    "nr=RandomForestRegressor() \n",
    "params_attr_nr_RFR={'n_estimators':[100],'max_features':[60],'max_depth':[20]}#, \n",
    "params_attr_nr=params_attr_nr_RFR#{'rfr':params_attr_nr_RFR}\n",
    "\n",
    "#nr=Pipeline([('poly_trans', PolynomialFeatures()),('lrr', Ridge())])\n",
    "#params_attr_nr_lrr={'alpha': np.linspace(10, 11, 1), 'normalize':[False]}\n",
    "#params_attr_nr_polytrans={'degree': [2],'interaction_only':[True]} \n",
    "#params_attr_nr={'poly_trans':params_attr_nr_polytrans, 'lrr':params_attr_nr_lrr}\n",
    "\n",
    "\n",
    "params_attr={'lreg':params_attr_lr,'nreg':params_attr_nr}#'lr':lr,'nr':nr,\n",
    "\n",
    "pipe_attr_est=Pipeline([('scaling', MaxAbsScaler()), ('ensemble', EnssembleReg(lr,nr))])#\n",
    "params_attr_pipe_est={'ensemble':params_attr}\n",
    "\n",
    "\n",
    "param_attr_grid = param_factory(params_attr_pipe_est,[],{})\n",
    "params_attr_gscv={'estimator':pipe_attr_est,\n",
    "                  'param_grid':param_attr_grid,\n",
    "                  'cv':20,\n",
    "                  'n_jobs':40,\n",
    "                  'verbose':1,\n",
    "                 'scoring': 'r2'}\n",
    "\n",
    "params_attr_rscv={'estimator':pipe_attr_est,\n",
    "                  'param_distributions':param_attr_grid,\n",
    "                  'cv':10,\n",
    "                  'n_jobs':40,\n",
    "                  'verbose':1,\n",
    "                  'random_state':42,\n",
    "                  'n_iter':50,\n",
    "                  'scoring': 'r2'}\n",
    "\n",
    "\n",
    "#========\n",
    "attribute_model = Pipeline([('toDF', ToDataFrame()),('f_attr',features_attr), ('est_attr_gscv',gscv(**params_attr_gscv))])\n",
    "#attribute_model = Pipeline([('toDF', ToDataFrame()),('f_attr',features_attr), ('est_attr_rscv',rscv(**params_attr_rscv))])#\n",
    "#attribute_model = Pipeline([('toDF', ToDataFrame()),('f_attr',features_attr), ('estimator',pipe_attr_est)])#\n",
    "\n",
    "attribute_model.fit(X_train,y_train)\n",
    "pickle_save(attribute_model, filename_model_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('memory', None)\n",
      "('steps', [('scaling', MaxAbsScaler()), ('ensemble', EnssembleReg(lreg=LinearRegression(), nreg=RandomForestRegressor()))])\n",
      "('verbose', False)\n",
      "('scaling', MaxAbsScaler())\n",
      "('ensemble', EnssembleReg(lreg=LinearRegression(), nreg=RandomForestRegressor()))\n",
      "('scaling__copy', True)\n",
      "('ensemble__lreg__copy_X', True)\n",
      "('ensemble__lreg__fit_intercept', True)\n",
      "('ensemble__lreg__n_jobs', None)\n",
      "('ensemble__lreg__normalize', False)\n",
      "('ensemble__lreg', LinearRegression())\n",
      "('ensemble__nreg__bootstrap', True)\n",
      "('ensemble__nreg__ccp_alpha', 0.0)\n",
      "('ensemble__nreg__criterion', 'mse')\n",
      "('ensemble__nreg__max_depth', None)\n",
      "('ensemble__nreg__max_features', 'auto')\n",
      "('ensemble__nreg__max_leaf_nodes', None)\n",
      "('ensemble__nreg__max_samples', None)\n",
      "('ensemble__nreg__min_impurity_decrease', 0.0)\n",
      "('ensemble__nreg__min_impurity_split', None)\n",
      "('ensemble__nreg__min_samples_leaf', 1)\n",
      "('ensemble__nreg__min_samples_split', 2)\n",
      "('ensemble__nreg__min_weight_fraction_leaf', 0.0)\n",
      "('ensemble__nreg__n_estimators', 100)\n",
      "('ensemble__nreg__n_jobs', None)\n",
      "('ensemble__nreg__oob_score', False)\n",
      "('ensemble__nreg__random_state', None)\n",
      "('ensemble__nreg__verbose', 0)\n",
      "('ensemble__nreg__warm_start', False)\n",
      "('ensemble__nreg', RandomForestRegressor())\n",
      "('ensemble__params', {})\n"
     ]
    }
   ],
   "source": [
    "#========Load saved model\n",
    "#attribute_model = pickle_load(filename_model_attr)\n",
    "\n",
    "#print(param_attr_grid)\n",
    "print(*[(k,v) for k,v in pipe_attr_est.get_params(deep=True).items()],sep='\\n')\n",
    "#print(params_attr_pipe_est)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================\n",
      "Your score: 0.9557\n",
      "==================\n"
     ]
    }
   ],
   "source": [
    "grader.score('ml__attribute_model', attribute_model.predict)  # Edit to appropriate name\n",
    "#model_attr=pickle_load(filename_model_attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "## Question 6: full_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "So far we have only built models based on individual features.  Now we will build an ensemble regressor that averages together the estimates of the four previous regressors.\n",
    "\n",
    "In order to use the existing models as input to a predictor, we will have to turn them into transformers; a predictor can only be in the final step of a pipeline. Build a custom `ModelTransformer` class that takes a predictor as an argument. When `fit` is called, the predictor should be fit. When `transform` is called, the predictor's `predict` method should be called, and its results returned as the transformation.\n",
    "\n",
    "Note that the output of the `transform` method should be a 2-D array with a single column in order for it to work well with the scikit-learn pipeline. If you're using NumPy arrays, you can use `.reshape(-1, 1)` to create a column vector. If you are just using Python lists, you will want a list of lists of single elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "class ModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        # What needs to be done here?\n",
    "        self.model=model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit the stored predictor.\n",
    "        # Question: what should be returned?\n",
    "        self.model.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        # Use predict on the stored predictor as a \"transformation\".\n",
    "        # Be sure to return a 2-D array.\n",
    "        y=self.model.predict(X)\n",
    "        return np.array(y).reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Let's now test it out on our `city_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_model=pickle_load(filename_model_city)\n",
    "model_latlong=pickle_load(filename_model_latlong)\n",
    "model_cat=pickle_load(filename_model_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_trans = ModelTransformer(city_model)\n",
    "city_trans.fit(data, star_ratings)\n",
    "X_t = city_trans.transform(data[:5])\n",
    "\n",
    "# Check that the transformation output is a 2-D array with one column\n",
    "grader.check(np.array(X_t).shape[-1] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.array(city_model.predict(data[:5]))\n",
    "\n",
    "# Check that the transformation output is the same as the model's predictions\n",
    "grader.check((y_pred.reshape(-1, 1) == X_t).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Create an instance of `ModelTransformer` for each of the previous four models. Combine these together in a single feature matrix with a\n",
    "[`FeatureUnion`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "city_trans = ModelTransformer(city_model)\n",
    "city_latlong = ModelTransformer(model_latlong)\n",
    "city_cat = ModelTransformer(model_cat)\n",
    "city_attr = ModelTransformer(attribute_model)\n",
    "\n",
    "\n",
    "model_union = FeatureUnion([('city', city_trans),\n",
    "                      ('location', city_latlong),\n",
    "                      ('category', city_cat),\n",
    "                      ('attribute', city_attr)\n",
    "        # FeatureUnion uses the same syntax as Pipeline\n",
    "    ])\n",
    "\n",
    "full_model=Pipeline([('model_union', model_union), ('lr', LinearRegression())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Our `FeatureUnion` object should return a feature matrix with four columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_union.fit(data,star_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37938 | [3.5, 4.0, 4.0, 4.5, 4.0, 1.5, 2.0, 2.5, 4.5, 3.5]\n",
      "Fitting 50 folds for each of 236 candidates, totalling 11800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend LokyBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:   20.4s\n",
      "[Parallel(n_jobs=50)]: Done 700 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=50)]: Done 1150 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=50)]: Done 1700 tasks      | elapsed:   41.8s\n",
      "[Parallel(n_jobs=50)]: Done 2350 tasks      | elapsed:   52.3s\n",
      "[Parallel(n_jobs=50)]: Done 3100 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=50)]: Done 3950 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=50)]: Done 4900 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=50)]: Done 5950 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=50)]: Done 7100 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=50)]: Done 8350 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=50)]: Done 9700 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=50)]: Done 11150 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=50)]: Done 11800 out of 11800 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37938 | [3.5, 4.0, 4.0, 4.5, 4.0, 1.5, 2.0, 2.5, 4.5, 3.5]\n",
      "Fitting 20 folds for each of 22 candidates, totalling 440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend LokyBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=50)]: Done 440 out of 440 | elapsed:   10.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37938 | [3.5, 4.0, 4.0, 4.5, 4.0, 1.5, 2.0, 2.5, 4.5, 3.5]\n",
      "Fitting 20 folds for each of 1 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=40)]: Using backend LokyBackend with 40 concurrent workers.\n",
      "[Parallel(n_jobs=40)]: Done   4 out of  20 | elapsed:  3.3min remaining: 13.4min\n",
      "[Parallel(n_jobs=40)]: Done  20 out of  20 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params -> \n",
      " reg_nr: \n",
      "{'max_depth': 20, 'max_features': 60, 'n_estimators': 100} & \n",
      "reg_lr: \n",
      "{}\n",
      "EnsReg: Start\n",
      "EnsReg: End\n"
     ]
    }
   ],
   "source": [
    "full_model.fit(data, star_ratings)\n",
    "X_t = model_union.transform(data[:5])\n",
    "\n",
    "# Transformed data should have 5 rows and 4 columns\n",
    "grader.check(X_t.shape == (5, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "Finally, use a pipeline to combine the feature union with a linear regression (or another model) to weight the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": null
   },
   "outputs": [],
   "source": [
    "grader.score('ml__full_model', full_model.predict)  # Edit to appropriate name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "**Extension:** Try a non-linear model such as [`RandomForestRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor) to blend the predictions of the four models. Are you able to get better results? If so, what do you think it's learning how to do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": null
   },
   "source": [
    "*Copyright &copy; 2021 Pragmatic Institute. This content is licensed solely for personal use. Redistribution or publication of this material is strictly prohibited.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tdi-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "nbclean": true,
  "vscode": {
   "interpreter": {
    "hash": "88bd797a4ea5b1ee644acf700150ab245ebc1e0047003f44e0d6bdca99f40f46"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
